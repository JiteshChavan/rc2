{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798ea072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FinalLayer (nn.Module):\n",
    "    \"\"\"\n",
    "    Final layer of the backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        scale, shift = self.adaLN_modulation(y).chunk(2, dim=-1) # 2x (B, C)\n",
    "        x = modulate(self.norm_final(x), shift, scale) # x = x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1) -> (B, T, C)\n",
    "        x = self.linear(x) # (B, T, C) - > (B, T, patch_size * patch_size * out_channels)\n",
    "        return x\n",
    "\n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9724c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = FinalLayer(7, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b179bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.randn(1, 3, 7), torch.randn(1, 7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2040df0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0447, -0.0520, -0.0628,  0.8552])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class activation(nn.Module):\n",
    "    def __init__(self, activation_layer= nn.GELU):\n",
    "        super().__init__()\n",
    "        self.act = activation_layer\n",
    "    \n",
    "    def forward (self, x):\n",
    "        return self.act(x)\n",
    "\n",
    "x = torch.randn(4)\n",
    "a = activation(activation_layer=nn.GELU(approximate=\"tanh\"))\n",
    "a(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ab2cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7365e-05],\n",
       "        [-1.8660e-03]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class GatedMLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fan_in: int,\n",
    "            fan_h: int = None,\n",
    "            fan_out: int = None,\n",
    "            act_layer = lambda:nn.GELU(approximate=\"tanh\"),\n",
    "            drop: float = 0.0,\n",
    "            bias: bool = True,\n",
    "    )-> None:\n",
    "        super().__init__()\n",
    "        fan_out = fan_out or fan_in # stores first truth value\n",
    "        fan_h = fan_h or fan_in\n",
    "        self.fc1 = nn.Linear(fan_in, 2*fan_h, bias=bias)\n",
    "        self.fc2 = nn.Linear(fan_h, fan_out, bias=bias)\n",
    "        self.act_layer = act_layer()\n",
    "    \n",
    "    def forward(self, x:Tensor)-> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x, scale = x.chunk(2, dim=-1)\n",
    "        x = self.act_layer(x) * scale\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# works! : when init calls approx_gelu(), it instantiates the GELU object\n",
    "approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "m = GatedMLP(1, 2, act_layer=approx_gelu, drop=0, bias=False)\n",
    "x = torch.randn(2, 1)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4c7884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.<lambda>()>, torch.nn.modules.activation.GELU)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_gelu, nn.GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6ac6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = nn.Conv2d(3, 3, 4, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09ca9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65645fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 4, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bab42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 48]), torch.Size([3, 48]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = x.view (x.shape[0], -1)\n",
    "k.shape, x.flatten(1, -1).shape\n",
    "\n",
    "x.view(x.shape[0], -1).shape, x.flatten(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d7f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import PatchEmbed\n",
    "a = PatchEmbed(32, 2, 4, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0abaf4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.num_patches**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012373a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d15c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.bias._noinit = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bd1d9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not hasattr(a.bias, \"_noinit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "684d6dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=3, bias=True)\n",
      "Linear(in_features=3, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for k,p in a.named_parameters():\n",
    "    print (a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19e7f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(a, nn.Linear):\n",
    "    nn.init.constant_(a.weight, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f884314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[16., 16., 16.],\n",
       "        [16., 16., 16.],\n",
       "        [16., 16., 16.]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9058f738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1373,  0.2918, -0.5081],\n",
       "        [ 0.3046,  0.5191,  0.0401],\n",
       "        [-0.1881, -0.4669, -0.4744]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "nn.init.kaiming_uniform_(a.weight, a=math.sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e74a4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1373,  0.2918, -0.5081],\n",
       "        [ 0.3046,  0.5191,  0.0401],\n",
       "        [-0.1881, -0.4669, -0.4744]], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b828d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a.weight /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8188ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0687,  0.1459, -0.2540],\n",
       "        [ 0.1523,  0.2596,  0.0200],\n",
       "        [-0.0941, -0.2334, -0.2372]], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b34d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention var:0.034645210951566696, Mamba var: 0.00042521519935689867\n"
     ]
    }
   ],
   "source": [
    "from CrossFusionMamba import CrossFusionMamba\n",
    "from CrossAttentionFusion import CrossAttentionFusion\n",
    "import torch\n",
    "fuser_mamba = CrossFusionMamba(768, 20, 32)\n",
    "fuser_att = CrossAttentionFusion(768)\n",
    "a = torch.randn(1, 3, 384)\n",
    "b = torch.randn(1, 3, 384)\n",
    "fuser_att.to('cuda')\n",
    "fuser_mamba.to('cuda')\n",
    "\n",
    "b=b.to('cuda')\n",
    "a=a.to('cuda')\n",
    "\n",
    "att = fuser_att(a,b)\n",
    "mamba = fuser_mamba(a,b)\n",
    "\n",
    "print(f\"Attention var:{att.var()}, Mamba var: {mamba.var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7288192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.modules.mamba_simple import CondMamba\n",
    "x = CondMamba(d_model=100, d_cond=100).to('cuda')\n",
    "a =torch.randn(2,10,100).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69436603",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x(a,a[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1b7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0015, device='cuda:0', grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "out.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1284103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from CrossAttentionFusion import CrossAttentionFusion\n",
    "from mamba_ssm.modules.mamba_simple import CondMamba\n",
    "x = CondMamba(d_model=64, d_cond=100).to('cuda')\n",
    "c = CrossAttentionFusion(64)\n",
    "a =torch.randn(2,10,100).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab7c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45568, 10304)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sum(p.numel()for p in x.parameters())\n",
    "b = sum(p.numel()for p in c.parameters())\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d804be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YAY\n",
      "BEFORENEW PASS\n",
      "\n",
      "YAY\n",
      "AFTER NEW PASS\n",
      "\n",
      "YAY\n",
      "BEFORENEW PASS\n",
      "\n",
      "YAY\n",
      "AFTER NEW PASS\n",
      "ArceeCond parameters are :592 and base cond parameters656\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm.modules.mamba_simple import CondMamba, ArceeCondMamba\n",
    "\n",
    "data = torch.randn(2, 10, 4).to('cuda')\n",
    "x = ArceeCondMamba(d_model=4, d_cond=4).to('cuda')\n",
    "y = ArceeCondMamba(d_model=4, d_cond=4).to('cuda')\n",
    "\n",
    "x1 = CondMamba(d_model=4, d_cond=4).to('cuda')\n",
    "\n",
    "\n",
    "activation = x(data, cond_emb=data[:,-1,:])\n",
    "activation = y(activation, cond_emb=activation[:,-1,:])\n",
    "loss = activation\n",
    "loss.mean().backward()\n",
    "\n",
    "for name, param in x.named_parameters():\n",
    "    if param.grad is None:\n",
    "        print(f\"[NO GRAD] {name}\")\n",
    "\n",
    "print (f\"ArceeCond parameters are :{sum(p.numel() for p in x1.parameters())} and base cond parameters{sum(p.numel() for p in x.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744b30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10., 10., 10.],\n",
      "        [10., 10., 10., 10.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 4, requires_grad=True)\n",
    "y = x[..., None].expand(-1, -1, 10)\n",
    "z = y.sum()\n",
    "\n",
    "# Reset gradients properly\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cond_proj = nn.Linear(4, 8)\n",
    "\n",
    "    def forward(self, cond_emb):\n",
    "        #cond_emb = self.cond_proj(cond_emb)[..., None].expand(-1, -1, 10)\n",
    "        cond_emb = self.cond_proj(cond_emb)[..., None].repeat(1, 1, 10)\n",
    "        return cond_emb.sum()\n",
    "\n",
    "model = Test().cuda()\n",
    "x = torch.randn(2, 4, device=\"cuda\", requires_grad=True)\n",
    "loss = model(x)\n",
    "loss.backward()\n",
    "\n",
    "print(model.cond_proj.weight.grad is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd424b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import gradcheck\n",
    "import torch.nn.functional as F\n",
    "B, D_cond, D_inner, L = 2, 3, 4, 5\n",
    "cond_emb = torch.randn(B, D_cond, dtype=torch.double, requires_grad=True)\n",
    "cond_proj_weight = torch.randn(D_inner, D_cond, dtype=torch.double, requires_grad=True)\n",
    "cond_proj_bias = torch.randn(D_inner, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "def f(cond_emb):\n",
    "    init_states = F.linear(cond_emb, cond_proj_weight, cond_proj_bias)\n",
    "    return init_states.unsqueeze(-1).expand(-1, -1, L).sum()  # simple loss\n",
    "\n",
    "gradcheck(f, (cond_emb,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dce2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
